---
title: topic models for synchronic & diachronic corpus exploration
description: 'A simple workflow from annotated corpus to topic model, with a focus on the exploratory utility of topic models.'
author: ''
date: '2018-02-26'
slug: static-and-historical-approaches-to-topic-modeling
tags: ['rstats','topic model', 'corpus ling']
output:
  blogdown::html_page:
    toc: yes
bibliography: biblio.bib
link-citations: yes
banner: banners/topic.jpg
---


<div id="TOC">
<ul>
<li><a href="#topic-models-and-text-structures">Topic models and text structures</a></li>
<li><a href="#synchronic-application">Synchronic application</a></li>
<li><a href="#diachronic-application">Diachronic application</a></li>
<li><a href="#topic-clusters">Topic clusters</a></li>
<li><a href="#quick-summary">quick summary</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>This post outlines a fairly simple workflow from annotated corpus to topic model, with a focus on the exploratory utility of topic models. We first consider some text structures relevant to topic modeling in R, and then demonstrate some approaches to visualizing model results, including variation in topic prevalence over time for a diachronic corpus. Lastly, we consider methods for visualizing relatedness among topics.</p>
<p>For demonstration purposes, we utilize a set of corpora made avaiable via the <code>corpusdatr</code> package.</p>
<pre class="r"><code>library(tidyverse)
library(topicmodels)
library(tidytext)
library(spacyr)</code></pre>
<pre class="r"><code>library(corpusdatr)#devtools::install_github(&quot;jaytimm/corpusdatr&quot;)
library(corpuslingr)#devtools::install_github(&quot;jaytimm/corpuslingr&quot;)</code></pre>
<div id="topic-models-and-text-structures" class="section level2">
<h2>Topic models and text structures</h2>
<p>Very quickly, topic modeling is an unsupervised text classification methodology in which</p>
<ul>
<li>documents are modeled as composites of topics, and</li>
<li>topics are modeled as composites of features/words.</li>
</ul>
<p>The approach to topic modeling employed here is latent Dirichlet allocation (LDA); to fit our models, we use the <code>LDA</code> function from the <code>topicmodels</code> package <span class="citation">(Hornik and Grün <a href="#ref-hornik2011topicmodels">2011</a>)</span>.</p>
<p>Like most topic modeling functions, the <code>topicmodels::LDA</code> function requires a <code>DocumentTermMatrix</code> as input. Informally, a DTM is simply a data structure in which each document in a corpus is represented in terms of its constitutent features and their frequencies (where features could be defined in terms of lemma, token, n-gram, keyphrase, etc).</p>
<p>As a data structure in R, however, a <code>DocumentTermMatrix</code> is awkward and not especially intuitive to work with. Conveniently, the <code>cast_sparse</code> function from the <code>tidytext</code> package allows us to get from informal DTM to formal DTM quite easily.</p>
</div>
<div id="synchronic-application" class="section level2">
<h2>Synchronic application</h2>
<p>In our first example, we investigate topics in the annotated Slate Magazine corpus (ca 1996-2000, 1K texts, 1m words), available as <code>cdr_slate_ann</code> from the <code>corpusdatr</code> package. The corpus has been annotated using the <code>spacyr</code> package, and is functionally a synchronic (or static) corpus by virtue of not containing publication date information.</p>
<p>Using the <code>clr_get_freq</code> function from the <code>corpuslingr</code> package, then, we create a DTM of the corpus using the lemma as our feature unit, and limit feature composition to nouns and entities.</p>
<pre class="r"><code>dtm &lt;- corpusdatr::cdr_slate_ann %&gt;%
  spacyr::entity_consolidate() %&gt;%
  filter(tag %in% c(&quot;NN&quot;, &quot;NNS&quot;) | pos ==&#39;ENTITY&#39;)%&gt;%
  corpuslingr::clr_get_freq(agg_var=c(&#39;doc_id&#39;,&#39;lemma&#39;),
                            toupper=FALSE)%&gt;%
  arrange(doc_id)</code></pre>
<p>Example portion of text as a DTM:</p>
<pre><code>##   doc_id   lemma txtf docf
## 1      1   noise   14   25
## 2      1     lip    9   24
## 3      1    talk    7  150
## 4      1    walk    6   24
## 5      1     one    5  874
## 6      1 service    5  177</code></pre>
<p>Per the output of <code>clr_get_freq</code>, we can filter out features with extreme document frequencies (ie, features with limited utility in classificaion). We then convert the DTM to a formal DTM using the <code>cast_sparse</code> function.</p>
<pre class="r"><code>static_DTM &lt;- dtm%&gt;%
  filter(docf &lt; 500 &amp; docf &gt; 5)%&gt;%
  tidytext::cast_sparse(row=doc_id,column=lemma,value=txtf)</code></pre>
<p>Which has the following structure:</p>
<pre><code>## Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   ..@ i       : int [1:161352] 0 18 20 104 191 197 229 254 280 291 ...
##   ..@ p       : int [1:5435] 0 25 49 199 223 400 540 596 1001 1067 ...
##   ..@ Dim     : int [1:2] 1000 5434
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:1000] &quot;1&quot; &quot;10&quot; &quot;100&quot; &quot;1000&quot; ...
##   .. ..$ : chr [1:5434] &quot;noise&quot; &quot;lip&quot; &quot;talk&quot; &quot;walk&quot; ...
##   ..@ x       : num [1:161352] 14 1 1 1 1 1 1 1 1 2 ...
##   ..@ factors : list()</code></pre>
<p>Lastly, we fit the model, specifying an eight topic solution:</p>
<pre class="r"><code>static_topic &lt;- topicmodels::LDA(static_DTM, 
                                 k = 8, 
                                 control=list(seed=12)) #11</code></pre>
<p>We extract the <code>terms</code> object from the <code>LDA</code> output using the <code>topicmodels::posterior</code> function; output includes the posterior probabilities of the terms for each topic. Focusing on the six highest probability terms per topic, the plot below summarizes model results for the Slate Magazine corpus (ca 1996-2000).</p>
<pre class="r"><code>library(ggthemes)
topicmodels::posterior(static_topic)$terms %&gt;%
  data.frame() %&gt;%
  mutate(topics = row.names(.))%&gt;%
  gather(key=&quot;term&quot;,value=&quot;beta&quot;, noise:wool) %&gt;%
  group_by(topics) %&gt;%
  top_n(6, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topics, beta)%&gt;%
  mutate(order = row_number(), 
         term=factor(paste(order,term,sep=&quot;_&quot;), 
                     levels = paste(order, term, sep = &quot;_&quot;)), 
         topics = as.character(topics))%&gt;%
  ggplot(aes(x=term, 
             y=beta, 
             fill=topics)) + 
    geom_col(show.legend = FALSE) +  
    facet_wrap(~topics, scales = &quot;free_y&quot;, ncol = 2) +
    scale_x_discrete(labels = function(x) gsub(&quot;^.*_&quot;, &quot;&quot;, x))+
    theme_fivethirtyeight()+ 
    scale_fill_stata() +
    coord_flip()+
    theme(plot.title = element_text(size=14)) +
    labs(title=&quot;Topic composition by feature&quot;) #</code></pre>
<p><img src="/post/2018-01-07-static-and-historical-approaches-to-topic-modeling_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>So, some curious times, the close of the 20th century: A lame-duck president in some hot water, presidential primaries, a war abroad. The model seems to paint a fairly clear picture of the socio-political happenings of the time period, and provides a nice macro-vantage from which to view/explore corpus content.</p>
</div>
<div id="diachronic-application" class="section level2">
<h2>Diachronic application</h2>
<p>Next we explore topics in a diachronic corpus, and demonstrate a straightforward approach to visualizing variation in topic prevalence over time. Here we use the <code>cdr_gnews_historical</code> corpus from the <code>corpusdatr</code> package for demonstration purposes.</p>
<p>
<h4>
Corpus and some descriptives
</h4>
</p>
<p>The corpus is comprised of web-based news articles published during a three-week time period (11-27/17 to 12/20/17). Articles were retrieved using my <code>quicknews</code> package, which leverages Goggle News’ RSS feed to direct search, and annotated using the <code>spacyr</code> package.</p>
<p>For the sake of avoiding copyright issues, each constituent article in the corpus is reduced to a document-term matrix (DTM). The corpus is comprised of ~1,500 texts, ~1.3 million words, and ~200 unique media sources.</p>
<p>Example corpus metadata:</p>
<pre class="r"><code>head(cdr_gnews_meta)[1:4]</code></pre>
<pre><code>##   doc_id   pubdates          source
## 1      1 2017-11-27  New York Times
## 2      2 2017-11-27  New York Times
## 3      3 2017-11-27 Washington Post
## 4      4 2017-11-27             CNN
## 5      5 2017-11-27             CNN
## 6      6 2017-11-27 Washington Post
##                                                                      titles
## 1         2 Bosses Show Up to Lead the Consumer Financial Protection Bureau
## 2                                    Meghan Markle Is Going to Make History
## 3 Trump could personally benefit from last-minute change to Senate tax bill
## 4                           Melania Trump unveils White House holiday decor
## 5        Trump&#39;s latest conspiracy? The &#39;Access Hollywood&#39; tape was a fake!
## 6                  Trump attacks media in his first post-Thanksgiving tweet</code></pre>
<p>Some basic corpus descriptives:</p>
<pre class="r"><code>cdr_gnews_meta%&gt;%
  group_by(pubdates) %&gt;%
  summarize_at(vars(docN),funs(sum))%&gt;%
  ggplot(aes(x=pubdates, group = 1)) +
  geom_line(aes(y=docN),
            size=1.25, 
            color = &#39;steelblue&#39;) +
  labs(title=&quot;Daily corpus size&quot;, 
       subtitle = &quot;11-27-17 to 12-20-17&quot;)</code></pre>
<p><img src="/post/2018-01-07-static-and-historical-approaches-to-topic-modeling_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The corpus is comprised predominantly of articles from the Washington Post, CNN, and the New York Times. It is unclear if these sources generate the most content, or if this is a bias of the news aggregator, or if these sites care less about folks scraping content from their sites.</p>
<pre class="r"><code>cdr_gnews_meta %&gt;%
  group_by(source) %&gt;%
  summarize_at(vars(docN),funs(sum))%&gt;%
  top_n(10,docN)%&gt;%
  ggplot(aes(x=reorder(source, docN), y=docN)) + 
  geom_col(width=.65, fill=&#39;steelblue&#39;) +  
  coord_flip()+
  labs(title=&quot;Top ten news sources by text frequency&quot;, 
       subtitle = &quot;11-27-17 to 12-20-17&quot;)</code></pre>
<p><img src="/post/2018-01-07-static-and-historical-approaches-to-topic-modeling_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>
<h4>
Topic Model
</h4>
</p>
<p>We follow the same procedure to fitting the topic model as we did previously:</p>
<pre class="r"><code>hist_topic &lt;- corpusdatr::cdr_gnews_historical %&gt;%
  filter(tag %in% c(&quot;NN&quot;, &quot;NNS&quot;) | pos ==&#39;ENTITY&#39;)%&gt;%
  group_by(doc_id,lemma) %&gt;%
  summarize_at(vars(freq),funs(sum))%&gt;%
  tidytext::cast_sparse(row=doc_id,column=lemma,value=freq)%&gt;%
  topicmodels::LDA(., 
                   k = 12, 
                   control = list(verbose = 0, seed=999))</code></pre>
<p>Extract the topic summary of the model:</p>
<pre class="r"><code>topic_summary &lt;- data.frame(topicmodels::terms(hist_topic,7)) %&gt;%
  gather(key=&#39;topic&#39;,value=&#39;val&#39;,Topic.1:Topic.12) %&gt;%
  group_by(topic)%&gt;%
  summarize (dims = paste(val,collapse=&#39;, &#39;))%&gt;%
  mutate(topic = as.numeric(gsub(&#39;Topic.&#39;,&#39;&#39;,topic)))%&gt;%
  arrange(topic)</code></pre>
<p>Topics over the three week period:</p>
<pre><code>## # A tibble: 12 x 1
##    dims                                                          
##    &lt;chr&gt;                                                         
##  1 tax, bill, Senate, Republicans, House, vote, rate             
##  2 Moore, Alabama, woman, voter, election, Republican, Trump     
##  3 woman, allegation, harassment, story, statement, time, people 
##  4 film, time, first, one, people, movie, way                    
##  5 North Korea, missile, snow, U.S., weapon, North Korean, report
##  6 company, deal, Fox, market, price, time, Disney               
##  7 EU, deal, government, trade, Yankees, time, Britain           
##  8 fire, wildfire, home, man, photo, Ventura, people             
##  9 CNN, Yemen, people, time, child, police, family               
## 10 game, team, time, coach, player, NFL, first                   
## 11 Trump, president, Russia, Trump_&#39;s, official, Mueller, Flynn  
## 12 police, Jerusalem, Israel, capital, city, death, people</code></pre>
<p>So a busy three weeks. The special Senate election in Alabama (and surrounding controversey), wildfires in California, North Korea, the Mueller investigation, tax reform, the #MeToo movement.</p>
<p>
<h4>
Topic prevalence historically
</h4>
</p>
<p>In order to quantify the prevalence of these topics over time, we shift focus from topic composition in terms of words/features to document composition in terms of topics. So, we first extract the posterior probabilities of the topics for each document; then we join corpus metadata and topic summary details.</p>
<pre class="r"><code>hist_beta &lt;- topicmodels::posterior(hist_topic)$topics %&gt;%
  data.frame() %&gt;%
  mutate(doc_id = row.names(.))%&gt;%
  arrange(as.numeric(doc_id))%&gt;%
  left_join(cdr_gnews_meta) %&gt;%
  gather(key=&quot;topic&quot;,value=&quot;val&quot;,X1:X12) %&gt;%
  mutate(topic = as.numeric(gsub(&#39;X&#39;,&#39;&#39;,topic)))%&gt;%
  left_join(topic_summary)</code></pre>
<p>Based on this set of model results, each document in our corpus can be represented as a composite of the sixteen topics summarized above; topic composites for an example set of texts are illustrated in the figure below. Per the figure, text 183 is comprised (in varying degrees) of topics 2, 3, 4, and 9.</p>
<pre class="r"><code>hist_beta %&gt;%
  filter(doc_id %in% c(&#39;183&#39;, &#39;631&#39;,&#39;896&#39;)) %&gt;%
  ggplot(aes(x=reorder(paste(topic,dims, sep=&quot; - &quot;), -topic), 
             y=val,  
             fill = dims)) +
  geom_col(width=.85) +  
  coord_flip()+
  scale_fill_stata() +
  theme(axis.text.x = element_text(angle = 90))+
  xlab (&quot;topic&quot;) + ylab(&quot;beta&quot;) +
  facet_wrap(~doc_id)+
  labs(title=&quot;Text by topic&quot;) +
  theme(legend.position=&quot;none&quot;)</code></pre>
<p><img src="/post/2018-01-07-static-and-historical-approaches-to-topic-modeling_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Topic prevalence over time, then, is simply the aggregate of these topic probabilities for each document by publication date.</p>
<pre class="r"><code>agg_hist_beta &lt;- hist_beta %&gt;%
  group_by(pubdates,topic,dims) %&gt;% 
  summarize_at(vars(val),funs(sum))%&gt;%
  ungroup()</code></pre>
<p>Finally, we plot the results. The size of plot points represents aggregate posterior probabilities, which can be interpreted as the likelihood that some article <strong>a</strong> written on day <strong>d</strong> was about some topic <strong>z</strong>.</p>
<p>The top six words associated with each topic are displayed as well. For a relatively small corpus (comprised of a wide range of content), the plot provides a nice overview of variation in topic prevalence over time.</p>
<pre class="r"><code>p &lt;- ggplot(agg_hist_beta) +
     geom_point(aes(x = pubdates, 
                    y = reorder(topic,-topic), 
                    size = val, 
                    color=dims)) +
  theme_fivethirtyeight() + 
  scale_color_stata() 

p + geom_text(data = agg_hist_beta[agg_hist_beta$pubdates == &quot;2017-11-27&quot;,], 
     aes(x = pubdates, y = reorder(topic,-topic), label = dims), 
     vjust=-1,
     hjust=0) +
  labs(title=&quot;Topic prevalence over time&quot;, 
       subtitle=&quot;11-27-2017 to 12-20-2017&quot;)+
  theme(legend.position=&quot;none&quot;, 
        plot.title = element_text(size=14))</code></pre>
<p><img src="/post/2018-01-07-static-and-historical-approaches-to-topic-modeling_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="topic-clusters" class="section level2">
<h2>Topic clusters</h2>
<p>Lastly, we consider the relationship among topics in the <code>cdr_gnews_historical</code> corpus via cluster analysis. The first step in this process is to create a correlation matrix of the beta values for constituent topic features.</p>
<pre class="r"><code>cor_mat &lt;- data.frame(posterior(hist_topic)$topics)%&gt;%
   `colnames&lt;-`(paste(topic_summary$topic,topic_summary$dims, sep=&quot; - &quot;)) %&gt;%
    cor(.)</code></pre>
<p>Next, we compute the distances (ie, dissimilarities) between topic-pairs, and perform hierarchical clustering analysis on the resulting matrix. We use the <code>ggdendro</code> package to plot results. Per plot below, some intuitive relationships, some less so.</p>
<pre class="r"><code>library(ggdendro)
hclust(dist(cor_mat)) %&gt;%
  ggdendrogram(., rotate=TRUE) + 
  theme_fivethirtyeight()</code></pre>
<p><img src="/post/2018-01-07-static-and-historical-approaches-to-topic-modeling_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
<div id="quick-summary" class="section level2">
<h2>quick summary</h2>
<p>A brief outline for quick topic modeling, with some different applications for synchronic and diachronic corpora. For a smarter discussion of underlying assumptions and maths, see <span class="citation">Hornik and Grün (<a href="#ref-hornik2011topicmodels">2011</a>)</span>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-hornik2011topicmodels">
<p>Hornik, Kurt, and Bettina Grün. 2011. “Topicmodels: An R Package for Fitting Topic Models.” <em>Journal of Statistical Software</em> 40 (13). American Statistical Association: 1–30.</p>
</div>
</div>
</div>
