---
title: 'covid19 & some computational-corpus linguistics' 
date: '2020-05-12'
slug: corp-comp-ling-covid19
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
tags:
  - rstats
  - Twitter
  - 116th Congress
bibliography: biblio.bib
link-citations: yes
banner: banners/covid_semantics.png
description: 'Some methods for working with controlled vocabularies & multi-word expressions while corpus ling-ing with Twitter data.'
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/datatables-binding/datatables.js"></script>
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>

<div id="TOC">
<ul>
<li><a href="#a-covid19-lexicon">A COVID19 lexicon</a></li>
<li><a href="#congressional-twitter-corpus-2020">Congressional Twitter Corpus (2020)</a></li>
<li><a href="#text2vec-framework-for-nlp"><code>text2vec</code> framework for NLP ::</a></li>
<li><a href="#tokens-tokenizers">Tokens &amp; tokenizers</a></li>
<li><a href="#multi-word-expressions-controlled-vocabularies">Multi-word expressions &amp; controlled vocabularies</a></li>
<li><a href="#dtms-prevalence-of-covid19-related-concepts">DTMs &amp; prevalence of COVID19-related concepts</a></li>
<li><a href="#glove-model-covid19-semantic-space">GloVe model &amp; COVID19 semantic space</a></li>
<li><a href="#networks-lexical-co-occurrence">Networks &amp; lexical co-occurrence</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
</div>

<p><strong>A SHORT COURSE IN COMPUTATIONAL-CORPUS LINGUISTICS</strong> – using the R library <code>text2vec</code> – with a focus on working with (a controlled vocabulary of) multi-word expressions. Here we consider a Twitter corpus comprised of all tweets generated by the <strong>535 voting members of the US Congress</strong> during the second session of the 116th Congress.</p>
<p>More specifically, we consider <strong>the usage of COVID19-related terms</strong> as a function of both time and party affiliation; we also investigate <strong>the conceptual relatedness</strong> of COVID19-relevant terms using a GloVe model and multi-dimensional scaling.</p>
<p>A cache of scalable &amp; efficient methodologies for some common corpus-based tasks.</p>
<div id="a-covid19-lexicon" class="section level2">
<h2>A COVID19 lexicon</h2>
<pre class="r"><code>library(tidyverse)
tweets_dir &lt;- &#39;/home/jtimm/jt_work/GitHub/git_projects/us_lawmaker_tweets_2020/&#39;
covid_dir &lt;- &#39;/home/jtimm/jt_work/GitHub/git_projects/A-covid19-lexicon/&#39;
setwd(covid_dir)
dictionary &lt;-  readxl::read_xlsx (&#39;covid_glossary_w_variants.xlsx&#39;) %&gt;%
  filter(category != &#39;race-ethnicity&#39;) %&gt;%
  ungroup()</code></pre>
<p>I have collated some COVID19-related terms from a few resources, most notably, this Yale Medicine <a href="https://www.yalemedicine.org/stories/covid-19-glossary/">glossary</a>. Per this resource, each term has been categorized as one of the following:</p>
<pre class="r"><code>unique(dictionary$category)</code></pre>
<pre><code>## [1] &quot;cv&quot;                &quot;interventions&quot;     &quot;medical_response&quot; 
## [4] &quot;prevention&quot;        &quot;socio-political&quot;   &quot;spread_of_disease&quot;
## [7] &quot;transmission&quot;</code></pre>
<p>As I have added terms, I have tried to fit them within this classification framework. I have also added a <code>socio-political</code> category to capture some of the civil-liberties-based rhetoric/protesting happening in the US in response to stay-at-home orders, as well as stimulus legislation, etc. <a href="https://github.com/jaytimm/A-covid19-lexicon">COVID19 vocabulary as xlsx file</a>. A good start, but could certainly be developed.</p>
<p>The table below illustrates the structure of the vocabulary for two COVID19-related concepts: <code>ANTIVIRAL</code> and <code>HAND-HYGIENE</code>. So, the <code>descriptor_name</code> column represents the higher-level concept; <code>term_name</code> column reflects the different (inflectional or orthographical) ways the concept can manifest in text. The actual form of the descriptor/concept is arbitrary.</p>
<p>So, as we move towards identifying/extracting COIV19-related terms from Twitter text, this vocabulary gives us the option to aggregate over terms to the higher-level concept (or descriptor). Some academic fields refer to this as the process of normalization.</p>
<pre class="r"><code>dictionary %&gt;% 
  filter(descriptor_name %in% c(&#39;antiviral&#39;, &#39;hand-hygiene&#39;)) %&gt;%
  group_by(category, descriptor_name) %&gt;%
  summarize(term_names = paste0(term_name, collapse = &#39; | &#39;))  %&gt;%
  DT::datatable(rownames = FALSE, options = list(dom = &#39;t&#39;))</code></pre>
<div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","data":[["interventions","prevention"],["antiviral","hand-hygiene"],["anti viral | anti virals | anti_viral | anti_virals | anti-viral | anti-virals | antiviral | antivirals","hand hygiene | hand_hygiene | hand-hygiene | handhygiene"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>category<\/th>\n      <th>descriptor_name<\/th>\n      <th>term_names<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="congressional-twitter-corpus-2020" class="section level2">
<h2>Congressional Twitter Corpus (2020)</h2>
<p>Again, our data set is a Twitter corpus comprised of all tweets generated by the <strong>535 voting members of the US Congress</strong> during the second session of the 116th Congress. Code for extracting/building/updating the corpus using the R package <code>rweet</code> is available <a href="https://github.com/jaytimm/us_lawmaker_tweets_2020/blob/master/get_tweets.Rmd">here</a>, and the actual corpus as TIF/xlsx is available <a href="https://github.com/jaytimm/us_lawmaker_tweets_2020/tree/master/tweets">here</a>.</p>
<pre class="r"><code>setwd(paste0(tweets_dir, &#39;tweets&#39;))
tweets &lt;- readxl::read_xlsx(&#39;us_lawmaker_tweets_full_2020-05-26.xlsx&#39;) %&gt;%
  mutate(created_at = as.Date(created_at, format = &quot;%Y-%B-%d&quot;))</code></pre>
<p>Corpus composition:</p>
<pre class="r"><code>data.frame(tweets = format(nrow(tweets), big.mark = &#39;,&#39;), 
           tokens = format(sum(tokenizers::count_words(tweets$text)), 
                           big.mark = &#39;,&#39;)) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">tweets</th>
<th align="left">tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">157,420</td>
<td align="left">5,650,645</td>
</tr>
</tbody>
</table>
<p>Next, we quickly grab some details about US lawmakers from <a href="https://theunitedstates.io/">the united states project</a>. The Twitter corpus and lawmaker detail data sets can then be joined via Twitter handle.</p>
<pre class="r"><code>leg_dets &lt;- &#39;https://theunitedstates.io/congress-legislators/legislators-current.csv&#39;
twitters &lt;- read.csv((url(leg_dets)), stringsAsFactors = FALSE) %&gt;%
  rename (state_abbrev = state, district_code = district)

tweets1 &lt;- tweets %&gt;%
  mutate(screen_name = toupper(screen_name)) %&gt;%
  left_join(twitters %&gt;% 
              mutate(twitter = toupper(twitter)),
            by = c(&#39;screen_name&#39; = &#39;twitter&#39;))</code></pre>
<p><strong>Some sample tweets:</strong></p>
<div id="htmlwidget-2" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"filter":"none","data":[["Carper","Reed","Scott"],["DE","RI","FL"],["2020-04-28","2020-03-22","2020-03-19"],["In a democracy, there must be a smooth, peaceful transition of power from one administration to the next. And candidates need to have a plan to do that.","Testing, testing, testing.\nThe Trump Admin’s epic failure to secure &amp;amp; deploy #coronavirus testing kits continues to hinder response &amp;amp; containment. Everyone should have access to testing &amp;amp; every hospital should be granted access to #PPE. We must speed assistance to those in need.","Right now, Congress needs to focus on the Americans that are hurt most by the #Coronavirus.\n\nCongress needs to break up the stimulus package and focus on getting relief to our small businesses, hourly workers &amp;amp; individuals whose livelihoods depend on tips. https://t.co/HzXHiZUlFj"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>last_name<\/th>\n      <th>state_abbrev<\/th>\n      <th>created_at<\/th>\n      <th>text<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p><br></p>
<hr />
</div>
<div id="text2vec-framework-for-nlp" class="section level2">
<h2><code>text2vec</code> framework for NLP ::</h2>
<p><code>text2vec</code> is a beast of a <a href="http://text2vec.org/">text analysis R library</a>. Here, we walk-through the building of some common text structures relevant to many downstream applications – using our congressional Twitter corpus. As <code>text2vec</code> implements R6 objects (a mystery to me), the framework is a bit funky. So, we present some hacks, etc. here, specifically for working with multi-word expressions – in the larger context of building document-term matrices, term-co-occurrence matrices, GloVe models &amp; co-occurrence-based graph structures.</p>
<p>With the ultimate goal of investigating (1) some historical- and party-affiliation-based variation in the use of COVID19-related terms on Twitter, and (2) the conceptual relatedness of COVID19-related terms</p>
<hr />
</div>
<div id="tokens-tokenizers" class="section level2">
<h2>Tokens &amp; tokenizers</h2>
<p><code>text2vec</code>, like other text analysis frameworks, operates on a <code>token object</code>, which for a single document/tweet looks like the following:</p>
<pre class="r"><code>tokenizers::tokenize_ptb(tweets$text[2], lowercase = TRUE)</code></pre>
<pre><code>## [[1]]
##  [1] &quot;ohioans&quot;           &quot;:&quot;                 &quot;request&quot;          
##  [4] &quot;a&quot;                 &quot;mail-in&quot;           &quot;ballot&quot;           
##  [7] &quot;today&quot;             &quot;from&quot;              &quot;the&quot;              
## [10] &quot;secretary&quot;         &quot;of&quot;                &quot;state&quot;            
## [13] &quot;to&quot;                &quot;ensure&quot;            &quot;your&quot;             
## [16] &quot;vote&quot;              &quot;is&quot;                &quot;counted&quot;          
## [19] &quot;in&quot;                &quot;ohio&quot;              &quot;&#39;s&quot;               
## [22] &quot;primary&quot;           &quot;election.&quot;         &quot;the&quot;              
## [25] &quot;deadline&quot;          &quot;to&quot;                &quot;postmark&quot;         
## [28] &quot;your&quot;              &quot;ballot&quot;            &quot;is&quot;               
## [31] &quot;monday.&quot;           &quot;https&quot;             &quot;:&quot;                
## [34] &quot;//t.co/gkdascowqc&quot;</code></pre>
<p>Tokenization, even for English, is a non-trivial task. The <code>tokenize_ptb</code> function from the <code>tokenizers</code> package is pretty good (which is based on the Penn Treebank model). But there are still two instances above, eg, in which sentence-final punctuation is not tokenized: <code>election.</code> &amp; <code>monday.</code>. So, when we go to build word-level models, <code>election</code> &amp; <code>election.</code>, eg, will be treated distinctly.</p>
<p>This bothers me. The code below sorts this and other issues out. Resulting/re-built text can then be fed to any simple space-based tokenizer, and things will be clean. <em>Tokenize &gt; clean tokens &gt; rebuild text &gt; re-tokenize</em>.</p>
<pre class="r"><code>## tokenizer --
t1 &lt;- tokenizers::tokenize_ptb(tweets$text, lowercase = TRUE)
## Remove punct
t2 &lt;- lapply(t1, gsub, 
             pattern = &#39;([a-z0-9])([[:punct:]])&#39;, 
             replacement = &#39;\\1 \\2&#39;) 
t3 &lt;- lapply(t2, gsub, 
             pattern = &#39;([[:punct:]])([a-z0-9])&#39;, 
             replacement = &#39;\\1 \\2&#39;) 

t4 &lt;- lapply(t3, paste0, collapse = &#39; &#39;)
## Re-build
tweets$word_text &lt;- unlist(t4)</code></pre>
</div>
<div id="multi-word-expressions-controlled-vocabularies" class="section level2">
<h2>Multi-word expressions &amp; controlled vocabularies</h2>
<p>Units of meaning often (ie, almost always) span multiple words and multiple grammatical categories. Here we briefly consider some supervised approaches to tricking tokenizers (and specifically <code>text2vec</code>) into treating a controlled vocabulary of multi-word expressions as single-units-of-meaning.</p>
<div id="some-multi-word-hacks" class="section level3">
<h3>§ Some multi-word hacks</h3>
<p>The spelling &amp; inflectional variants of the COVID19-related concept <code>FLATTEN THE CURVE</code> are presented below:</p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">term_names</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">flatten the curve | flatten_the_curve | flatten-the-curve | flattening the curve | flattening_the_curve | flattening-the-curve | flatteningthecurve | flattenthecurve</td>
</tr>
</tbody>
</table>
<p>So, if a lawmaker on Twitter refers to the concept <code>FLATTEN THE CURVE</code> as <code>flattenthecurve</code>, without any spaces (&amp; presumably prefixed with a hash tag), a space-based (or word-based) tokenizer will do right by the analyst investigating multi-word expressions. The same goes for <code>flatten-the-curve</code> and <code>flatten_the_curve</code>.</p>
<p>The form <code>flatten the curve</code>, however, will be tokenized as <code>flatten</code> and <code>the</code> and <code>curve</code>. Which is not helpful. Basically, we want to phrasify these three individual tokens as a single token. Such that in downstream applications, <code>flatten the curve</code> and <code>flattenthecurve</code>, eg, are (or can be) treated as instantiations of the same conceptual category.</p>
<p>The <code>Collocations</code> function/model from the <code>text2vec</code> package enables an unsupervised approach to identifying multi-word expressions, and results can be used to update token objects such that <code>flatten</code> <code>the</code> <code>curve</code> becomes <code>flatten-the-curve</code>. If <code>flatten</code> + <code>the</code> + <code>curve</code> is identified as an expression per the model.</p>
<p>Here, however, we are interested in a supervised (or controlled) approach, ie, we have our own multi-word lexicon of COVID19-related terms that we want phrasified. <code>text2vec</code> does not provide a straightforward way to do this. So, here we present a simple (albeit extended) hack.</p>
<pre class="r"><code>multi_word_expressions &lt;- subset(dictionary, grepl(&#39; &#39;, term_name))
sep = &#39; &#39;
mas_que_dos &lt;- subset(multi_word_expressions, grepl(&#39; [a-z0-9]* &#39;, term_name))</code></pre>
<p>First: <code>text2vec::Collocations</code> builds out phrases in a piecemeal fashion. Long story short: in order to identify (or phrasify) <code>flatten the curve</code> as a multi-word expression, it must first identify (or phrasify), eg, <code>flatten the</code>. Then <code>flatten-the</code> and <code>curve</code> can be phrasified as <code>flatten-the-curve</code>. So, for multi-word expressions &gt; 2, we have to build out some component parts. Some multi-word expressions in the COVID19 vocabulary &gt;2 words:</p>
<pre><code>##  [1] &quot;flattening the curve&quot;          &quot;drive thru tests&quot;             
##  [3] &quot;personal protective equipment&quot; &quot;flatten the curve&quot;            
##  [5] &quot;front line worker&quot;             &quot;great american comeback&quot;      
##  [7] &quot;global economic cirsis&quot;        &quot;return to work&quot;               
##  [9] &quot;high risk population&quot;          &quot;god bless america&quot;</code></pre>
<p>A simple function for extracting component 2-word phrases from multi-word expressions &gt;2:</p>
<pre class="r"><code>new_two_grams &lt;- lapply(mas_que_dos$term_name, function(x) {
  regmatches(x, 
             gregexpr(&quot;[^ ]+ [^ ]+&quot;, # sep = &#39; &#39;
                      x, 
                      perl=TRUE)
  )[[1]] }) %&gt;%
  unlist() %&gt;%
  unique()</code></pre>
<p>A look at the “pieces” of our mutli-word expressions composed of more than two words:</p>
<pre><code>##  [1] &quot;sars cov&quot;            &quot;drive through&quot;       &quot;drive thru&quot;         
##  [4] &quot;personal protective&quot; &quot;flatten the&quot;         &quot;flattening the&quot;     
##  [7] &quot;front line&quot;          &quot;high risk&quot;           &quot;shelter in&quot;         
## [10] &quot;long term&quot;           &quot;stay at&quot;             &quot;home order&quot;         
## [13] &quot;home orders&quot;         &quot;wash your&quot;           &quot;wear a&quot;             
## [16] &quot;work from&quot;           &quot;working from&quot;        &quot;dont bankrupt&quot;      
## [19] &quot;global economic&quot;     &quot;global health&quot;       &quot;god bless&quot;          
## [22] &quot;great american&quot;      &quot;made in&quot;             &quot;open up&quot;            
## [25] &quot;paycheck protection&quot; &quot;re open&quot;             &quot;return to&quot;          
## [28] &quot;person to&quot;           &quot;person transmission&quot;</code></pre>
<p>Then we add these “pieces” to the full multi-word portion of the COVID19 lexicon.</p>
<pre class="r"><code>multi_word_expressions_replace &lt;- gsub(&#39; &#39;, sep, multi_word_expressions$term_name)
multi_word_expressions_replace &lt;- c(multi_word_expressions_replace,
                                    new_two_grams )</code></pre>
</div>
<div id="some-text2vec-primitives" class="section level3">
<h3>§ Some <code>text2vec</code> primitives</h3>
<p>Before we can dupe <code>text2vec</code> into phrasifying our multi-word COVID19 terms, we first need to build two basic <code>text2vec</code> (data) structures: an <code>itoken</code> object (or iterator) &amp; a vocabulary object. The former containing (among other things) a generic tokens object. Again, see <a href="http://text2vec.org/">this vignette</a> for more technical details. Regardless of your <code>text2vec</code> objectives, these will (almost) always be your first two opening moves.</p>
<pre class="r"><code>mo &lt;- text2vec::itoken(tweets$word_text, 
                       preprocessor = tolower,
                       tokenizer = text2vec::space_tokenizer, 
                       n_chunks = 1,
                       ids = tweets$status_id) 
  
vocab &lt;- text2vec::create_vocabulary(mo, stopwords = character(0)) #tm::stopwords()</code></pre>
<p>Then we build a skeleton <code>Collocations</code> model per code below. But we never actually run the model.</p>
<pre class="r"><code>model &lt;- text2vec::Collocations$new(vocabulary = vocab, sep = sep) </code></pre>
<p>Instead, all we want to do is assign the parameter <code>model$.__enclos_env__$private$phrases</code> our list of multi-word expressions.</p>
<pre class="r"><code>model$.__enclos_env__$private$phrases &lt;- multi_word_expressions_replace</code></pre>
<p>Using this dummy <code>Collocations</code> model, we then <code>transform</code> the <code>itoken</code> object built above. Here, transform means updating the token object to account for multi-word expressions.</p>
<pre class="r"><code>it_phrases &lt;- model$transform(mo) 
term_vocab &lt;- text2vec::create_vocabulary(it_phrases) 
term_vocab1 &lt;- text2vec::prune_vocabulary(term_vocab, term_count_min = 2)

## HACK 
ats &lt;- attributes(term_vocab1)
term_vocab2 &lt;- subset(term_vocab1, grepl(&#39;^[A-Za-z]&#39;, term) &amp; nchar(term) &gt; 2)
t2v_vocab &lt;- term_vocab2
attributes(t2v_vocab) &lt;- ats
#egs &lt;- it_phrases$nextElem()$tokens
#egs1 &lt;- lapply(egs, paste0, collapse = &#39; &#39;)</code></pre>
<p>And now we can investigate frequencies for all forms included in the congressional Twitter corpus, including (but not limited to) our multi-word expressions.</p>
<pre class="r"><code>term_freqs &lt;- term_vocab2 %&gt;%
  left_join(dictionary , by = c(&#39;term&#39; = &#39;term_name&#39;))

descriptor_freq &lt;- term_freqs %&gt;%
  group_by(category, descriptor_name) %&gt;%
  summarize(term_freq = sum(term_count)) %&gt;%
  filter(!is.na(descriptor_name))</code></pre>
<p><strong>Some relative frequencies</strong> for spelling &amp; lexical variants for a sample of multi-word expressions from the COVID19 lexicon.</p>
<pre class="r"><code>term_freqs %&gt;%
  filter(descriptor_name %in% c(&#39;social-distancing&#39;, &#39;front-line-workers&#39;,
                                &#39;flatten-the-curve&#39;)) %&gt;%
  arrange(desc(term_count)) %&gt;%
  mutate(tf = paste0(term, &#39; (&#39;, term_count, &#39;)&#39;)) %&gt;%
  group_by(descriptor_name) %&gt;%
  summarize(relative = paste0(tf, collapse = &#39; | &#39;)) %&gt;%
  DT::datatable(rownames = FALSE, options = list(dom = &#39;t&#39;))</code></pre>
<div id="htmlwidget-3" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-3">{"x":{"filter":"none","data":[["flatten-the-curve","front-line-workers","social-distancing"],["flattenthecurve (362) | flatten the curve (140) | flattening the curve (51) | flatteningthecurve (7)","frontline workers (481) | front line workers (89) | frontline worker (7) | frontlineworkers (3)","social distancing (1345) | socialdistancing (153) | social distance (86) | socialdistance (13) | socially distant (10) | socially distanced (8) | social distanced (2)"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>descriptor_name<\/th>\n      <th>relative<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>So, the trickier/hackier part is complete. The <code>text2vec</code> vocabulary object now recognizes the multi-word expressions in our COVID19 lexicon as single units of meaning. And we can carry on.</p>
</div>
</div>
<div id="dtms-prevalence-of-covid19-related-concepts" class="section level2">
<h2>DTMs &amp; prevalence of COVID19-related concepts</h2>
<div id="document-term-matrix" class="section level3">
<h3>§ Document-term matrix</h3>
<p>DTMs can be leveraged in any number of ways, for any number of downstream linguistic applications. Here, we want to use the DTM to understand variation in the use of COVID19-related concepts as a function of time and political affiliation – the former a feature of the tweet; the latter a feature of the tweeter, ie, the US lawmaker.</p>
<p>The <code>text2vec::create_dtm</code> function requires an additional object, a <code>vocab_vectorizer</code>, which is derived from the <code>vocabulary</code> object discussed/created above. The <code>vocab_vectorizer</code> operates on the phrasified token object contained in <code>it_phrases</code>. Using my <code>lexvarsdatr</code> package, we then convert the DTM to a long data frame.</p>
<pre class="r"><code>vectorizer &lt;- text2vec::vocab_vectorizer(t2v_vocab)
dtm0 &lt;- text2vec::create_dtm(it_phrases, vectorizer)

dtm1 &lt;- lexvarsdatr::lvdr_get_closest(dtm0) ## other ways -- 
colnames(dtm1) &lt;- c(&#39;doc_id&#39;, &#39;term_name&#39;, &#39;count&#39;)
dtm2 &lt;- dtm1 %&gt;% inner_join(dictionary)</code></pre>
</div>
<div id="historical-prevalences" class="section level3">
<h3>§ Historical prevalences</h3>
<p>Per this new data structure, we can now add tweet metadata (here, date of creation) as well as tweeter details, eg, name, party affiliation, gender, etc.</p>
<pre class="r"><code>dtm3 &lt;- dtm2 %&gt;%
  ## Aggregate -- to descriptor_name
  group_by(doc_id, descriptor_name, category) %&gt;%
  summarize(count = sum(count)) %&gt;%
  ungroup() %&gt;%
  
  left_join(tweets %&gt;% select(status_id, screen_name, created_at) %&gt;%
            mutate(screen_name = toupper(screen_name)), by = c(&#39;doc_id&#39; = &#39;status_id&#39;)) %&gt;%
  left_join(twitters %&gt;% 
              select(full_name, type:district_code, twitter, party) %&gt;%
              mutate(twitter = toupper(twitter)), by = c(&#39;screen_name&#39; = &#39;twitter&#39;))</code></pre>
<p>We can now calculate the <strong>daily rate of reference</strong> for each concept included in the COVID19 lexicon. Here, the rate of reference for concept <strong>X</strong> is calculated as the # of tweets referring to concept <strong>X</strong> per 1K total tweets (for a given day).</p>
<pre class="r"><code>for_plot_historical &lt;- dtm3 %&gt;%
  group_by(created_at) %&gt;%
  mutate(daily_total = length(unique(doc_id))) %&gt;%
  
  group_by(descriptor_name, created_at, daily_total) %&gt;%
  summarize (n = n()) %&gt;%
  mutate(per_k_tweets = round(n/daily_total * 1000, 3)) %&gt;%
  ungroup()
  # mutate(cumulative_n = cumsum(n)) %&gt;%
  #   complete(created_at = seq.Date(from = min(created_at), 
  #                                  to = max(created_at), 
  #                                  by=&quot;day&quot;), 
  #          descriptor_name) %&gt;% fill(cumulative_n)

for_plot_historical$per_k_tweets[is.na(for_plot_historical$per_k_tweets)] &lt;- 0</code></pre>
</div>
<div id="historical-plot" class="section level3">
<h3>§ Historical plot</h3>
<p>The plot below illustrates daily rates of reference to several COVID19-related concepts since the beginning of March. Black lines represent 7-day moving averages.</p>
<pre class="r"><code>keeps &lt;- c(&#39;cares-act&#39;, &#39;flatten-the-curve&#39;, 
           &#39;stay-at-home&#39;, &#39;social-distancing&#39;,
           &#39;essential-workers&#39;, &#39;face-mask&#39;, 
           &#39;paycheck-protection-program&#39;, &#39;first-responders&#39;,
           &#39;ventilator&#39;)

### 
for_plot_historical %&gt;%
  filter(descriptor_name %in% keeps &amp;
           created_at &gt; &#39;2020-03-01&#39;) %&gt;%
  mutate(moving_n = zoo::rollmean(per_k_tweets, k = 7, fill = NA)) %&gt;%
  
  ggplot() +
  
  geom_bar(aes(x = created_at, 
             y= per_k_tweets, #cumulative_n, 
             fill = descriptor_name),
           stat=&quot;identity&quot;) +
  #geom_bar(stat=&quot;identity&quot;) +
  geom_line(aes(x = created_at,
                y = moving_n),
            color = &#39;black&#39;,
            #linetype = 2,
            size = .75) +
  theme_minimal() +
  ggthemes::scale_fill_stata() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_x_date(date_breaks = &#39;1 week&#39;, date_labels = &quot;%b %d&quot;) +
  theme(legend.position = &#39;none&#39;,
        legend.title = element_blank())  + 
  ylab(&#39;Daily reference rate (per 1K tweets)&#39;) +
  facet_wrap(~descriptor_name, scales = &#39;free_y&#39;) +
  labs(title = &#39;Daily reference rates to some COVID-related concepts since March 1st&#39;,
       subtitle = &#39;With 7-day rolling averages in black&#39;,
       caption = &#39;Source: Congressional Twitter Corpus, 2020&#39;)</code></pre>
<p><img src="/post/2020-05-12-corp-comp-ling-covid19_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>See <a href="https://www.jtimm.net/2020/03/28/a-global-pandemic-on-twitter/">this post</a> for an onomatalogical perspective on how reference to COVID19/coronavirus has changed among members of the US House since February.</p>
</div>
<div id="party-prevalences" class="section level3">
<h3>§ Party prevalences</h3>
<p>Next, we consider variation in reference to COVID19 concepts among US lawmakers as a function of party affiliation. For COVID19 concept <strong>X</strong>, then, prevalence is calculated as the quotient of (1) the rate of reference for concept <strong>X</strong> among Republicans and (2) the rate of reference for concept <strong>X</strong> among Democrats. We tweak this quotient some such that the number reflects a percentage increase.</p>
<p>For a similar analysis on how party affiliation influences how US House Reps refer to the President of the USA on Twitter, see <a href="https://www.jtimm.net/2019/08/20/referring-to-potus/">this post</a>.</p>
<pre class="r"><code>descriptor_by_party &lt;- dtm3 %&gt;%
  filter(party != &#39;Independent&#39;) %&gt;%
  group_by(party) %&gt;%
  mutate(party_total = length(unique(doc_id))) %&gt;%
  
  group_by(party, descriptor_name, party_total) %&gt;%
  summarize(n = n()) %&gt;%
  
  group_by(descriptor_name) %&gt;%
  mutate(descriptor_total = sum(n)) %&gt;%
  ungroup() %&gt;%
  filter(descriptor_total &gt; 40) %&gt;%

  mutate(per_1k = round(n/party_total * 1000, 2)) %&gt;%
  select(-n, -party_total) %&gt;%
  spread(party, per_1k) %&gt;%
  mutate(ratio = ifelse (Democrat &lt; Republican, 
                         (Republican/Democrat) - 1, 
                         (-Democrat/Republican) +1 )) %&gt;%
  mutate(ratio = round(ratio * 100, 2)) %&gt;%
  filter(!is.na(ratio) &amp; abs(ratio) &lt; 200)</code></pre>
<p>The table below illustrates several examples of the relative prevalences of COVID19-related concepts as a function of political affiliation. Interpreting table, eg: Republicans are ~30% more likely than Democrats to reference <code>american-dream</code> on Twitter.</p>
<table>
<thead>
<tr class="header">
<th align="left">descriptor_name</th>
<th align="right">descriptor_total</th>
<th align="right">Democrat</th>
<th align="right">Republican</th>
<th align="right">ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">american-dream</td>
<td align="right">112</td>
<td align="right">2.07</td>
<td align="right">2.71</td>
<td align="right">30.92</td>
</tr>
<tr class="even">
<td align="left">cares-act</td>
<td align="right">3476</td>
<td align="right">62.82</td>
<td align="right">86.22</td>
<td align="right">37.25</td>
</tr>
<tr class="odd">
<td align="left">civil-liberty</td>
<td align="right">75</td>
<td align="right">1.30</td>
<td align="right">1.95</td>
<td align="right">50.00</td>
</tr>
<tr class="even">
<td align="left">close-contact</td>
<td align="right">155</td>
<td align="right">4.04</td>
<td align="right">1.84</td>
<td align="right">-119.57</td>
</tr>
<tr class="odd">
<td align="left">community-spread</td>
<td align="right">54</td>
<td align="right">1.37</td>
<td align="right">0.70</td>
<td align="right">-95.71</td>
</tr>
<tr class="even">
<td align="left">coronavirus</td>
<td align="right">16397</td>
<td align="right">313.64</td>
<td align="right">378.67</td>
<td align="right">20.73</td>
</tr>
</tbody>
</table>
<p><strong>COVID19-concepts politicized</strong>. A Democrat-Republican continuum. Concepts in green are ~neutral. Certainly some interesting differences, that shed light on both ideology and constituency demographics.</p>
<pre class="r"><code>descriptor_by_party %&gt;%
  mutate(col1 = ifelse(ratio &gt; 0, &#39;red&#39;, &#39;blue&#39;)) %&gt;%
  mutate(col1 = ifelse(ratio &gt; -25 &amp; ratio &lt; 25, &#39;x&#39;, col1)) %&gt;%
  ggplot(aes(x=reorder(descriptor_name, 
                       ratio), 
             y=ratio, 
             label=descriptor_name,
             color = col1))  +
  geom_hline(yintercept = 25,
             linetype = 2, color = &#39;gray&#39;) +
  geom_hline(yintercept = -25,
             linetype = 2, color = &#39;gray&#39;) +
  geom_point(size= 1.5,
            color = &#39;darkgray&#39;) +
  geom_text(size=4, 
            hjust = 0, 
            nudge_y = 5)+
  annotate(&#39;text&#39; , y = -75, x = 20, label = &#39;DEMOCRAT&#39;) +
  annotate(&#39;text&#39; , y = 100, x = 10, label = &#39;REPUBLICAN&#39;) +
  ggthemes::scale_color_stata() +
  theme_minimal() +
  labs(title=&quot;COVID19-related concepts&quot;,
       subtitle = &#39;Prevalence by party affiliation&#39;) + ##?
  
  theme(legend.position = &quot;none&quot;,
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  xlab(&#39;&#39;) + ylab(&#39;% more likely&#39;)+
  ylim(-150, 200) +
  coord_flip()</code></pre>
<p><img src="/post/2020-05-12-corp-comp-ling-covid19_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
</div>
<div id="glove-model-covid19-semantic-space" class="section level2">
<h2>GloVe model &amp; COVID19 semantic space</h2>
<p>The next piece is to build a GloVe model to investigate semantic relatedness among concepts included in our COVID19 lexicon. The general workflow here is:</p>
<ol style="list-style-type: decimal">
<li>Build a term-co-occurrence matrix (TCM),</li>
<li>Build an <em>n</em>-dimensional GloVe model based on the TCM,</li>
<li>Further reduce GloVe dimensions via tSNE, PCA, or MDS,</li>
<li>Plot terms in a reduced 2D space.</li>
</ol>
<p>Here, we have the additional task of <strong>aggregating the TCM from terms to descriptors</strong> (or concepts), before building the GloVe model. The code below creates a simple table that crosswalks terms to concepts.</p>
<pre class="r"><code>term_vocab3 &lt;- term_vocab2 %&gt;%
  rename(term_name = term) %&gt;%
  left_join(dictionary) %&gt;%
  mutate(descriptor_name = ifelse(is.na(descriptor_name), 
                                  term_name, 
                                  descriptor_name),
           category = ifelse(is.na(category), 
                             &#39;other&#39;, 
                             category)) %&gt;%
  arrange(term_name)</code></pre>
<div id="term-co-occurrence-matrix" class="section level3">
<h3>§ Term-co-occurrence matrix</h3>
<p>Utilizing previously constructed <code>text2vec</code> primitives, we use the <code>text2vec::create_tcm</code> function to construct a term-co-occurrence matrix, specifying a context window-size of 5 x 5.</p>
<pre class="r"><code>tcm &lt;- text2vec::create_tcm(it = it_phrases,
                            vectorizer = vectorizer,
                            skip_grams_window = 5L)</code></pre>
<p>Then we implement the <code>lvdr_aggregate_matrix</code> function from the <code>lexvarsdatr</code> package to aggregate term vectors to a single descriptor vector (for forms included in the COVID19 lexicon).</p>
<pre class="r"><code>tcm &lt;- tcm[, order(colnames(tcm))]
tcm &lt;- tcm[order(rownames(tcm)), ]

tcm1 &lt;- lexvarsdatr::lvdr_aggregate_matrix(tfm = tcm, 
                                           group = term_vocab3$descriptor_name, 
                                           fun = &#39;sum&#39;)</code></pre>
<p>Dimensions of TCM:</p>
<pre><code>## [1] 42468 42468</code></pre>
</div>
<div id="glove-model" class="section level3">
<h3>§ GloVe model</h3>
<p>We specify GloVe model parameters via the <code>text2vec::GlobalVectors</code> function, and build term vectors using <code>fit_transform</code>. Vectors are comprised of n = 128 dimensions.</p>
<pre class="r"><code>set.seed(99)
glove &lt;- text2vec::GlobalVectors$new(rank = 128, 
                                     #vocabulary = row.names(tcm1), 
                                     x_max = 10)
  
wv_main &lt;- glove$fit_transform(tcm1, 
                               n_iter = 10, 
                               convergence_tol = 0.01)
  
wv_context &lt;- glove$components
glove_vectors &lt;- wv_main + t(wv_context)</code></pre>
</div>
<div id="semantic-conceptual-relatedness" class="section level3">
<h3>§ Semantic &amp; conceptual relatedness</h3>
<p>With GloVe vectors in tow, options abound. Here, we demonstrate two fairly straightforward applications. The first – a quick look at <strong>nearest neighbors</strong> for a set of COVID19-related concepts. Via cosine similarity and the <code>LSAfun::neighbors</code> function.</p>
<pre class="r"><code>eg_terms &lt;- c(&#39;stay-at-home&#39;, &#39;outbreak&#39;, 
              &#39;front-line-workers&#39;, &#39;vaccine&#39;,
              &#39;relief&#39; )

x &lt;- lapply(eg_terms, 
            LSAfun::neighbors, 
            glove_vectors, 
            n = 10)

names(x) &lt;- eg_terms</code></pre>
<p><strong>Top 10 nearest neighbors</strong> for <code>stay-at-home</code>, <code>outbreak</code>, <code>front-line-workers</code>, <code>vaccine</code> &amp; <code>relief</code>. So, despite a relatively small corpus, some fairly nice results.</p>
<pre><code>## $`stay-at-home`
##      stay-at-home          practice social-distancing flatten-the-curve 
##         1.0000000         0.5637298         0.5538846         0.4839460 
##              sick              stay             avoid           staying 
##         0.4760842         0.4450705         0.4363015         0.4090789 
##            unless               you 
##         0.4003368         0.3695571 
## 
## $outbreak
##    outbreak coronavirus    pandemic       covid   covidー19      crisis 
##   1.0000000   0.6587903   0.5642116   0.5348093   0.5225736   0.4993063 
##      spread    response       virus     impacts 
##   0.4792686   0.4679729   0.4618700   0.4608478 
## 
## $`front-line-workers`
## front-line-workers  essential-workers   first-responders                ppe 
##          1.0000000          0.5794326          0.5285931          0.4847675 
##             heroes      professionals          equipment             nurses 
##          0.4717064          0.4574281          0.4490630          0.4490211 
##          frontline            doctors 
##          0.4462532          0.4433965 
## 
## $vaccine
##     vaccine  treatments     develop   therapies   treatment development 
##   1.0000000   0.5759950   0.4578306   0.4238894   0.4060573   0.3863896 
##        easy       tests     genesis    research 
##   0.3790296   0.3439374   0.3273765   0.3251489 
## 
## $relief
##     relief    funding        aid    provide additional       bill    support 
##  1.0000000  0.6017136  0.5787071  0.5717479  0.5708550  0.5670038  0.5657665 
##    package assistance  cares-act 
##  0.5618101  0.5562318  0.5458088</code></pre>
<p>For a smarter approach to visualizing nearest neighbors, as well as visualizing lexical semantic change historically, see <a href="https://www.jtimm.net/2019/04/14/lexical-change-procrustes/">this post</a>. Or, a similar analysis <a href="https://github.com/jaytimm/google_ngrams_and_R">utilizing Google n-gram data</a>.</p>
<p>In the second application, we consider a <strong>two-dimensional perspective on the semantic relatedness</strong> of concepts included in our COVID19 lexicon. While we have built vectors for all forms attested in the Congressional Twitter Corpus, here we subset this matrix to just COVID concepts. Via classical multidimensional scaling, we project 128 features (per GloVe) into two-dimensional Euclidean space.</p>
<pre class="r"><code>set.seed(99)
keeps &lt;- descriptor_freq %&gt;% filter(term_freq &gt; 30)
glove1 &lt;- glove_vectors[rownames(glove_vectors) %in% 
                          unique(keeps$descriptor_name),]
sim_mat &lt;- text2vec::sim2(glove1, 
                             method = &quot;cosine&quot;, 
                             norm = &quot;l2&quot;)

# data set too small for tSNE ---

y1 &lt;- cmdscale(1-sim_mat, eig = TRUE, k = 2)$points %&gt;% 
  data.frame() %&gt;%
  mutate (descriptor_name = rownames(sim_mat)) %&gt;%
  left_join(dictionary %&gt;% distinct(descriptor_name, category)) </code></pre>
<p><strong>A semantic map</strong> of COVID19 concepts is presented below. Some intuitive structure for sure; some less so.</p>
<pre class="r"><code>y1 %&gt;% 
  ggplot(aes(X1,X2, label = descriptor_name)) +
  geom_point(aes(color = category), size = 3.5) +
  ggrepel::geom_text_repel(
    data  = y1,
    nudge_y =  0.025,
    segment.color = &quot;grey80&quot;,
    direction = &quot;y&quot;,
    hjust = 0, 
    size = 3 ) +
  ggthemes::scale_colour_stata() + 
  theme_minimal() +
  #theme_classic() +
  theme(legend.position = &quot;bottom&quot;,
        plot.title = element_text(size=14))+ 
  labs(title=&quot;COVID19-related concepts in 2D semantic space&quot;)</code></pre>
<p><img src="/post/2020-05-12-corp-comp-ling-covid19_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>See a similar application using co-occurrence and place-names to create a <a href="https://www.jtimm.net/2018/03/12/text-geography-and-semantic-space/">text-based map of the world</a>.</p>
</div>
</div>
<div id="networks-lexical-co-occurrence" class="section level2">
<h2>Networks &amp; lexical co-occurrence</h2>
<p>Lastly, we build &amp; visualize a co-occurrence network based on the previously constructed term-co-occurrence matrix. The <code>lexvarsdatr</code> package streamlines these processes, and enables straightforward extraction of sub-networks from large matrices. See <a href="https://github.com/jaytimm/lexvarsdatr">package description</a> for a more detailed discussion.</p>
<p>Below, we convert our count-based TCM to a <em>positive point-wise mutual information</em>-based matrix (via <code>lvdr_calc_ppmi</code>), and extract the 20 strongest collocates (via <code>lvdr_extract_network</code>) for five (cherry-picked) concepts included in the COVID19 lexicon.</p>
<pre class="r"><code>network &lt;- tcm1 %&gt;% 
  lexvarsdatr::lvdr_calc_ppmi(make_symmetric = TRUE) %&gt;%
  lexvarsdatr::lvdr_extract_network (
    target = c(&#39;contact-tracing&#39;, 
               &#39;flatten-the-curve&#39;,
               &#39;return-to-work&#39;,
               &#39;social-distancing&#39;,
               #&#39;remote-learning&#39;,
               &#39;drive-through-testing&#39;), 
    n = 20)</code></pre>
<p><strong>And then visualize</strong>:</p>
<pre class="r"><code>set.seed(66)
network %&gt;%
  tidygraph::as_tbl_graph() %&gt;%
  ggraph::ggraph() +
  
  ggraph::geom_edge_link(color = &#39;darkgray&#39;) + 
  ggraph::geom_node_point(aes(size = value, 
                              color = term,
                              shape = group)) +
  
  ggraph::geom_node_text(aes(label = toupper(label), 
                             filter = group == &#39;term&#39;), 
                             repel = TRUE, size = 4) +
  
  ggraph::geom_node_text(aes(label = tolower(label), 
                             filter = group == &#39;feature&#39;), 
                             repel = TRUE, size = 3) +
  ggthemes::scale_color_stata()+
  theme_minimal() +
  ggtitle(&#39;A COVID19 co-occurrence network&#39;) +
  theme(legend.position = &quot;none&quot;,
        plot.title = element_text(size=14))</code></pre>
<p><img src="/post/2020-05-12-corp-comp-ling-covid19_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>So, more of a resource/guide than a post-proper. Mostly an attempt on my part to collate some scattered methods. And a bit of an ode to<code>text2vec</code>.</p>
</div>
