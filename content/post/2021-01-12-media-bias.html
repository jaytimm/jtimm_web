---
title: media bias & shared news on Twitter
author: ''
date: '2021-01-29'
slug: media-bias
categories: []
tags: []
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
bibliography: biblio.bib
link-citations: yes
description: 'Stream-lininig url-extraction from tweets.'
banner: banners/url.png
---


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#tweet-set">Tweet-set</a></li>
<li><a href="#media-bias-data-set">Media bias data set</a></li>
<li><a href="#resolving-shortened-urls">Resolving shortened URLs</a></li>
<li><a href="#shared-news-media-sources">Shared news media sources</a></li>
<li><a href="#media-bias-tsne">Media bias &amp; tSNE</a></li>
<li><a href="#resources">Resources</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This post provides a brief description of methods for quantifying political bias of online news media based on the media-sharing habits of US lawmakers on Twitter. I have discussed this set of methods in <a href="https://www.jtimm.net/2018/11/03/twitter-political-ideology-and-the-115-us-senate/">a previous post</a>. Here, the focus is on a more streamlined (and multi-threaded) approach to <strong>resolving shortened URLs</strong> via the <code>quicknews</code> package. We also present unsupervised methods for visualizing media bias in two-dimensional space via tSNE, and compare results to the manually curated fact and bias checking online resource, <a href="https://mediabiasfactcheck.com/">Media Bias/Fact Check</a> (MBFC), with some fairly nice results.</p>
<pre class="r"><code>library(tidyverse)
localdir &lt;- &#39;/home/jtimm/jt_work/GitHub/data_sets&#39;
##  devtools::install_github(&quot;jaytimm/quicknews&quot;)</code></pre>
</div>
<div id="tweet-set" class="section level2">
<h2>Tweet-set</h2>
<p>The tweet-set used here was accessed via the <a href="https://tweetsets.library.gwu.edu/datasets">GWU Library</a>, and subsequently “hydrated” using the <a href="https://github.com/DocNow/hydrator">Hydrator</a> desktop application. Tweets were generated by members of the 116th House from 3 Jan 2019 to 7 May 2020. Subsequent analyses are based on a sample of 500 tweets/lawmaker containing shared URLs.</p>
<pre class="r"><code>setwd(localdir)
house_tweets &lt;- readRDS(&#39;house116-sample-urls.rds&#39;) %&gt;%
  filter(urls != &#39;&#39;)</code></pre>
</div>
<div id="media-bias-data-set" class="section level2">
<h2>Media bias data set</h2>
<p><a href="https://mediabiasfactcheck.com/">Media Bias/Fact Check</a> is a fact-checking organization that classifies online news sources along two dimensions: (1) political bias and (2) factuality. These two scores (for ~850 sources) have been extracted by <span class="citation">Baly et al. (<a href="#ref-baly:2020:ACL2020" role="doc-biblioref">2020</a>)</span>, and made available in tabular format <a href="https://github.com/ramybaly/News-Media-Reliability">here</a>.</p>
<pre class="r"><code>setwd(&#39;/home/jtimm/jt_work/GitHub/packages/quicknews/data-raw&#39;)
## emnlp18 &lt;- read.csv(&#39;emnlp18-corpus.tsv&#39;, sep = &#39;\t&#39;)
acl2020 &lt;- read.csv(&#39;acl2020-corpus.tsv&#39;, sep = &#39;\t&#39;)</code></pre>
<p>A sample of this data set is presented below.</p>
<pre class="r"><code>set.seed(221)
acl2020 %&gt;%
  group_by(fact, bias) %&gt;%
  sample_n(1) %&gt;%
  # ungroup() %&gt;%
  select(source_url_normalized, fact, bias) %&gt;%
  # spread(bias, source_url_normalized) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">source_url_normalized</th>
<th align="left">fact</th>
<th align="left">bias</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">wn.com</td>
<td align="left">high</td>
<td align="left">center</td>
</tr>
<tr class="even">
<td align="left">dailydot.com</td>
<td align="left">high</td>
<td align="left">left</td>
</tr>
<tr class="odd">
<td align="left">yellowhammernews.com</td>
<td align="left">high</td>
<td align="left">right</td>
</tr>
<tr class="even">
<td align="left">freakoutnation.com</td>
<td align="left">low</td>
<td align="left">left</td>
</tr>
<tr class="odd">
<td align="left">christianaction.org</td>
<td align="left">low</td>
<td align="left">right</td>
</tr>
<tr class="even">
<td align="left">wionews.com</td>
<td align="left">mixed</td>
<td align="left">center</td>
</tr>
<tr class="odd">
<td align="left">extranewsfeed.com</td>
<td align="left">mixed</td>
<td align="left">left</td>
</tr>
<tr class="even">
<td align="left">lifenews.com</td>
<td align="left">mixed</td>
<td align="left">right</td>
</tr>
</tbody>
</table>
</div>
<div id="resolving-shortened-urls" class="section level2">
<h2>Resolving shortened URLs</h2>
<p>The <a href="https://github.com/jaytimm/quicknews">quicknews</a> package is a collection of tools for navigating the online news landscape; here, we detail a simple workflow for researchers to use for multi-threaded URL un-shortening. As a three step process: (1) identify URLs that have been shortened via <code>qnews_clean_urls</code>, (2) split vector of URLs into multiple batches via <code>qnews_split_batches</code> for distribution across multiple cores, and (3) resolve shortened URLs via <code>qnews_unshorten_urls</code>.</p>
<pre class="r"><code>## step 1
shortened_urls &lt;- quicknews::qnews_clean_urls(url = house_tweets$urls) %&gt;%
  filter(is_short == 1) 

## step 2
batch_urls &lt;- shortened_urls %&gt;% quicknews::qnews_split_batches(n = 12)

## step 3
unshortened_urls &lt;- parallel::mclapply(lapply(batch_urls, &quot;[[&quot;, 1),
                                       quicknews::qnews_unshorten_urls,
                                       seconds = 10, 
                                       mc.cores = 12)

unshortened_urls1 &lt;- data.table::rbindlist(unshortened_urls)</code></pre>
</div>
<div id="shared-news-media-sources" class="section level2">
<h2>Shared news media sources</h2>
<p>Next, we update the original tweet-set with the resolved URLs from above; we also extract domain information from each shared link in our data set.</p>
<pre class="r"><code>full_tweets &lt;- house_tweets %&gt;%
  left_join(unshortened_urls1, by = c(&#39;urls&#39; = &#39;short_url&#39;)) %&gt;%
  mutate(long_url = ifelse(is.na(long_url), urls, long_url), 
         source = gsub(&#39;(http)(s)?(://)(www\\.)?&#39;, &#39;&#39;, long_url),
         source = gsub(&#39;/.*$&#39;, &#39;&#39;, source),
         user_screen_name = toupper(user_screen_name))  ###</code></pre>
<p>The list below details some less useful domains that we can remove from the data frame of shared URLs.</p>
<pre class="r"><code>junks &lt;-  c(&#39;facebook&#39;, &#39;lnkd.in&#39;,
            &#39;twitter&#39;, &#39;youtube&#39;,
            &#39;youtu\\.be&#39;, &#39;instagram&#39;,
            &#39;twimg&#39;, &#39;tumblr&#39;,
            &#39;google&#39;, &#39;medium&#39;,
            &#39;vimeo&#39;, &#39;\\.gov&#39;,
            &#39;actblue\\.com&#39;, &#39;bit\\.ly&#39;,
            &#39;ow\\.ly&#39;, &#39;timeout&#39;,
            &#39;myemail&#39;, &#39;apple.news&#39;,
            &#39;trib.al&#39;)

filt.tweets &lt;- full_tweets %&gt;%
  filter(!grepl(paste0(junks, collapse = &#39;|&#39;), long_url)) </code></pre>
<p>The table below summarizes some of the more frequently shared news media domains among lawmakers during the 116th congress. For good measure, domains are ranked by <code>% coverage</code>, which is the percentage of lawmakers that have shared a news link from a given domain in our data set. So, 94% (or 403/429) of House members shared content from <a href="https://thehill.com/">The Hill</a>, which compares to 49% for Fow News and only 15% for Breitbert.</p>
<pre class="r"><code>share.summary &lt;- filt.tweets %&gt;% 
  mutate(source = tolower(source)) %&gt;%
  group_by(source) %&gt;%
  summarize(n = n(), tweeters = length(unique(user_screen_name))) %&gt;%
  ungroup() %&gt;%
  mutate(cover = round(tweeters/429*100,1)) %&gt;%
  #left_join(acl2020, by = c(&#39;source&#39; = &#39;source_url_normalized&#39;)) %&gt;%
  arrange(desc(tweeters)) %&gt;%
  filter(tweeters &gt; 10) </code></pre>
<table>
<thead>
<tr class="header">
<th align="left">source</th>
<th align="right">n</th>
<th align="right">tweeters</th>
<th align="right">cover</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">thehill.com</td>
<td align="right">2977</td>
<td align="right">403</td>
<td align="right">93.9</td>
</tr>
<tr class="even">
<td align="left">washingtonpost.com</td>
<td align="right">4853</td>
<td align="right">384</td>
<td align="right">89.5</td>
</tr>
<tr class="odd">
<td align="left">politico.com</td>
<td align="right">1782</td>
<td align="right">354</td>
<td align="right">82.5</td>
</tr>
<tr class="even">
<td align="left">c-span.org</td>
<td align="right">1488</td>
<td align="right">346</td>
<td align="right">80.7</td>
</tr>
<tr class="odd">
<td align="left">nytimes.com</td>
<td align="right">4717</td>
<td align="right">342</td>
<td align="right">79.7</td>
</tr>
<tr class="even">
<td align="left">cnn.com</td>
<td align="right">1802</td>
<td align="right">323</td>
<td align="right">75.3</td>
</tr>
<tr class="odd">
<td align="left">usatoday.com</td>
<td align="right">889</td>
<td align="right">311</td>
<td align="right">72.5</td>
</tr>
<tr class="even">
<td align="left">cnbc.com</td>
<td align="right">973</td>
<td align="right">309</td>
<td align="right">72.0</td>
</tr>
<tr class="odd">
<td align="left">nbcnews.com</td>
<td align="right">1086</td>
<td align="right">282</td>
<td align="right">65.7</td>
</tr>
<tr class="even">
<td align="left">wsj.com</td>
<td align="right">1043</td>
<td align="right">277</td>
<td align="right">64.6</td>
</tr>
</tbody>
</table>
</div>
<div id="media-bias-tsne" class="section level2">
<h2>Media bias &amp; tSNE</h2>
<div id="build-matrix" class="section level3">
<h3>Build matrix</h3>
<p>To aggregate these data, we build a simple <code>domain-lawmaker matrix</code>, in which each domain/news organization is represented by the number of times each lawmaker has shared one of its news stories.</p>
<pre class="r"><code>ft1 &lt;- filt.tweets %&gt;%
  group_by(user_screen_name, source) %&gt;%
  count() %&gt;%
  filter(source %in% share.summary$source) %&gt;%
  tidytext::cast_sparse(row = &#39;source&#39;,
                        column = &#39;user_screen_name&#39;,
                        value = n)

ft2 &lt;- as.matrix(ft1) #%&gt;% Rtsne::normalize_input()</code></pre>
<p>Matrix top-left::</p>
<pre class="r"><code>ft2[1:5, 1:5]</code></pre>
<pre><code>##                   AUSTINSCOTTGA08 BENNIEGTHOMPSON BETTYMCCOLLUM04 BILLPASCRELL
## abcnews.go.com                  1               4               0            3
## airforcetimes.com               1               0               0            0
## ajc.com                         6               0               0            0
## bloomberg.com                   2               3               0            5
## c-span.org                      2               1               4            3
##                   BOBBYSCOTT
## abcnews.go.com             0
## airforcetimes.com          0
## ajc.com                    0
## bloomberg.com              2
## c-span.org                 1</code></pre>
</div>
<div id="tsne" class="section level3">
<h3>TSNE</h3>
<pre class="r"><code>set.seed(77) ## 9
tsne &lt;- Rtsne::Rtsne(X = ft2, check_duplicates = FALSE)
tsne_clean &lt;- data.frame(descriptor_name = rownames(ft1), tsne$Y) %&gt;% 
  #mutate(screen_name = toupper(descriptor_name)) %&gt;%
  left_join(acl2020, by = c(&#39;descriptor_name&#39; = &#39;source_url_normalized&#39;)) %&gt;%
  replace(is.na(.), &#39;x&#39;)</code></pre>
</div>
<div id="plot" class="section level3">
<h3>Plot</h3>
<p>Per figure below, the first dimension of the tSNE plot does a fairly nice job capturing differences in <strong>bias classifications</strong> as presented by <a href="https://mediabiasfactcheck.com/">Media Bias/Fact Check</a>, and results are generally intuitive. Factors underlying variation along the second dimension, however, are less clear, and do not appear to be capturing <strong>factuality</strong> in this case. <em>Note: news organizations indicated by orange Xs are not included in the MB/FC data set.</em></p>
<pre class="r"><code>split_pal &lt;- c(&#39;#3c811a&#39;, 
               &#39;#395f81&#39;, &#39;#9e5055&#39;,
               &#39;#e37e00&#39;)

tsne_clean %&gt;%
  ggplot(aes(X1, X2)) +
  geom_point(aes(col = bias, 
                 shape = fact),
             size = 3) +
  geom_text(aes(label = descriptor_name,
                col = bias,
                shape = fact), #
            size = 3, 
            check_overlap = TRUE) +
  theme_minimal() +
  theme(legend.position = &quot;bottom&quot;) +
  scale_color_manual(values = split_pal) +
  xlab(&#39;Dimension 1&#39;) + ylab(&#39;Dimension 2&#39;)+ 
  labs(title = &quot;Measuring political bias&quot;)</code></pre>
<p><img src="/post/2021-01-12-media-bias_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="bias-score-distributions" class="section level3">
<h3>Bias score distributions</h3>
<pre class="r"><code>tsne_clean %&gt;%
  ggplot() +
  geom_density(aes(X1, fill = bias),
               alpha = .4) +
  theme_minimal() +
  theme(legend.position = &quot;bottom&quot;) +
  scale_fill_manual(values = split_pal) +
  ggtitle(&#39;Media bias scores by MB/FC bias classification&#39;)</code></pre>
<p><img src="/post/2021-01-12-media-bias_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
</div>
<div id="resources" class="section level2 unnumbered">
<h2>Resources</h2>
<div id="refs" class="references">
<div id="ref-baly:2020:ACL2020">
<p>Baly, Ramy, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov, Ahmed Ali, James Glass, and Preslav Nakov. 2020. “What Was Written Vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context.” In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. ACL ’20.</p>
</div>
</div>
</div>
