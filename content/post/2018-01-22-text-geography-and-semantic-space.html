---
title: 'place from text: geography & distributional semantics'
description: 'Some different methodologies for exploring the geographical information found in text.'
author: ''
date: '2018-03-12'
slug: text-geography-and-semantic-space
tags: ['rstats', 'corpus ling', 'geo', 'LSA', 'MDS']
output:
  blogdown::html_page:
    toc: yes
    df_print: paged
banner: banners/geotext.png

---

<link href="/rmarkdown-libs/pagedtable/css/pagedtable.css" rel="stylesheet" />
<script src="/rmarkdown-libs/pagedtable/js/pagedtable.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/pymjs/pym.v1.js"></script>
<script src="/rmarkdown-libs/widgetframe-binding/widgetframe.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/datatables-binding/datatables.js"></script>
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>

<div id="TOC">
<ul>
<li><a href="#from-text-to-map">From text to map</a></li>
<li><a href="#corpus-search-and-context">Corpus search and context</a></li>
<li><a href="#lsa-mds-and-semantic-space">LSA, MDS, and semantic space</a></li>
<li><a href="#fin">FIN</a></li>
</ul>
</div>

<p>In this post, we demonstrate some different methodologies for exploring the geographical information found in text. First, we address some of the practical issues of extracting places/place-names from an annotated corpus, and demonstrate how to (1) map their geospatial distribution via geocoding and (2) append additional geographic detail to these locations via spatial joins.</p>
<p>We then consider how these locations “map” in semantic space by comparing context-based word embeddings for each location. Ultimately, the endgame is to investigate the extent to which geospatial proximity is reflected (or not) in distributional similarity in a corpus. In the process, we demonstrate some methods for getting from lexical co-occurrence to a 2D semantic map via latent semantic analysis (LSA) and classical multi-dimensional scaling (MDS).</p>
<pre class="r"><code>library(tidyverse)
library(ggthemes)
library(corpuslingr) #devtools::install_github(&quot;jaytimm/corpuslingr&quot;)
library(corpusdatr) #devtools::install_github(&quot;jaytimm/corpusdatr&quot;)
library(knitr)</code></pre>
<div id="from-text-to-map" class="section level2">
<h2>From text to map</h2>
<p>
<h3>
Slate corpus &amp; geopolitical entities
</h3>
</p>
<p>For demo purposes, we use the annotated Slate magazine corpus made available as <code>cdr_slate_ann</code> via the <code>corpusdatr</code>. Content of articles comprising the corpus is largely political in nature, so lots of reference to place and location, namely foreign and domestic political entities. The first task, then, is to get a rollcall of the geopolitical entities included in the corpus.</p>
<p>The Slate Magazine corpus has been annotated using the <code>spacyr</code> package, and contains named entity tags, including geopolitical entities (GPEs). Here we collapse multi-word entities (eg, “New” “York”) to single tokens (eg, “New_York”), and ready the corpus for search using <code>clr_set_corpus</code>.</p>
<pre class="r"><code>slate &lt;- corpusdatr::cdr_slate_ann %&gt;%
  spacyr::entity_consolidate() %&gt;%
  corpuslingr::clr_set_corpus(ent_as_tag=TRUE)</code></pre>
<p>Next, we obtain text and document frequencies for GPEs included in the corpus, and filter to only those occurring in 1% or greater of articles comprising the corpus.</p>
<pre class="r"><code>slate_gpe &lt;- slate %&gt;%
  bind_rows()%&gt;%
  filter(tag == &#39;NNGPE&#39;)%&gt;%
  corpuslingr::clr_get_freq(agg_var=&#39;lemma&#39;,toupper=TRUE) %&gt;%
  filter(txtf&gt;9 &amp; !grepl(&#39;US|USA|AMERICA|UNITED_STATES|THE_UNITED_STATES|U.S.|U.S.A&#39;,lemma))</code></pre>
<p>The most frequently referenced GPEs in the Slate corpus (not including the US):</p>
<table>
<thead>
<tr class="header">
<th align="left">lemma</th>
<th align="right">txtf</th>
<th align="right">docf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">WASHINGTON</td>
<td align="right">398</td>
<td align="right">230</td>
</tr>
<tr class="even">
<td align="left">KOSOVO</td>
<td align="right">298</td>
<td align="right">78</td>
</tr>
<tr class="odd">
<td align="left">CHINA</td>
<td align="right">262</td>
<td align="right">94</td>
</tr>
<tr class="even">
<td align="left">NEW_YORK</td>
<td align="right">222</td>
<td align="right">143</td>
</tr>
<tr class="odd">
<td align="left">ISRAEL</td>
<td align="right">204</td>
<td align="right">78</td>
</tr>
<tr class="even">
<td align="left">BRITAIN</td>
<td align="right">161</td>
<td align="right">85</td>
</tr>
</tbody>
</table>
<p>
<h3>
Geocoding
</h3>
</p>
<p>To visualize the geographical distribution of GPEs in the Slate Magazine corpus, we use the <code>geocode</code> function from the <code>ggmap</code> package to transform our corpus locations to lat/lon coordinates that can be mapped. While <code>ggmap</code> works best with proper addresses (eg, street, city, zip, etc), country and city names can be geolocated as well.</p>
<p>Note that while GPEs are geographical areas, this method approximates GPE location as a single point in lat/long space at the center (or centroid) of these areas. For our purposes here, this approximation is fine.</p>
<p>The following pipe geocodes the GPEs, removes GPEs that Google Maps cannot geocode, and transforms the new dataframe with lat/lon coordinates into an <code>sf</code> spatial object. The last step enables convenient mapping/geospatial processing within the <code>sf</code> framework.</p>
<pre class="r"><code>library(ggmap)
library(sf)

slate_gpe_geo &lt;- ggmap::geocode(slate_gpe$lemma, 
                                output = c(&quot;latlon&quot;), 
                                messaging = FALSE) %&gt;%
  bind_cols(slate_gpe)%&gt;%
  filter(complete.cases(.))%&gt;%
  sf::st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;), 
               crs = 4326)</code></pre>
<p>We then map the geolocated GPEs using the <code>leaflet</code> package; circle radius reflects frequency of occurrence in the slate corpus.</p>
<pre class="r"><code>library(leaflet)
library(widgetframe)

x &lt;- slate_gpe_geo %&gt;%
  leaflet(width=&quot;100%&quot;) %&gt;%
  setView(lng = -5, lat = 31, zoom = 2) %&gt;%
  addProviderTiles (&quot;CartoDB.Positron&quot;,
                    options = providerTileOptions (minZoom = 2, maxZoom = 4))%&gt;%
  addCircleMarkers(
    radius = ~txtf/25,
    stroke = FALSE, fillOpacity = .75,
    label=~lemma)

frameWidget(x)</code></pre>
<div id="htmlwidget-1" style="width:100%;height:480px;" class="widgetframe html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"url":"/post/2018-01-22-text-geography-and-semantic-space_files/figure-html//widgets/widget_unnamed-chunk-7.html","options":{"xdomain":"*","allowfullscreen":false,"lazyload":false}},"evals":[],"jsHooks":[]}</script>
<p>
<h3>
Spatial joins
</h3>
</p>
<p>The <code>spData</code> package conveniently makes available a variety of shapefiles/geopolitical polygons as <code>sf</code> objects, including a world country map. Having geocoded the GPEs, we can add features from this country map (eg, country, subregion, continent) to our GPE points via a spatial join. We use the <code>st_join</code> function from the <code>sf</code> package to accomplish this task.</p>
<pre class="r"><code>library(spData)
slate_gpe_details &lt;- sf::st_join(slate_gpe_geo, spData::world)</code></pre>
<p>Per the spatial join, we now have information regarding country, continent, and subregion for each GPE from the Slate Magazine corpus.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">lemma</th>
<th align="left">name_long</th>
<th align="left">continent</th>
<th align="left">subregion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>4</td>
<td align="left">ALBANIA</td>
<td align="left">Albania</td>
<td align="left">Europe</td>
<td align="left">Southern Europe</td>
</tr>
<tr class="even">
<td>5</td>
<td align="left">ARGENTINA</td>
<td align="left">Argentina</td>
<td align="left">South America</td>
<td align="left">South America</td>
</tr>
<tr class="odd">
<td>6</td>
<td align="left">ARIZONA</td>
<td align="left">United States</td>
<td align="left">North America</td>
<td align="left">Northern America</td>
</tr>
<tr class="even">
<td>7</td>
<td align="left">ARKANSAS</td>
<td align="left">United States</td>
<td align="left">North America</td>
<td align="left">Northern America</td>
</tr>
<tr class="odd">
<td>8</td>
<td align="left">ARLINGTON</td>
<td align="left">United States</td>
<td align="left">North America</td>
<td align="left">Northern America</td>
</tr>
<tr class="even">
<td>9</td>
<td align="left">ATHENS</td>
<td align="left">Greece</td>
<td align="left">Europe</td>
<td align="left">Southern Europe</td>
</tr>
</tbody>
</table>
<p>We can use this information, for example, to aggregate GPE text and document frequencies to the subregion level:</p>
<pre class="r"><code>slate_gpe_details %&gt;%
  data.frame()%&gt;%
  group_by(subregion) %&gt;%
  summarize (txtf=sum(txtf),docf=sum(docf))%&gt;%
  filter(subregion!=&#39;Northern America&#39;)%&gt;%
  ggplot(aes(x=docf, y=txtf)) + 
  geom_text(aes(label=toupper(subregion)), 
            size=3, 
            check_overlap = TRUE,
            hjust = &quot;inward&quot;)+
  labs(title = &quot;Document vs. text frequency for GPEs outside of Northern America&quot;, 
       subtitle=&quot;By Subregion&quot;)</code></pre>
<p><img src="/post/2018-01-22-text-geography-and-semantic-space_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="corpus-search-and-context" class="section level2">
<h2>Corpus search and context</h2>
<p>So, our next task is to map the GPEs in 2D (semantic) space by comparing context-based word embeddings for each location. <em>What does a map derived from patterns of lexical co-occurrence in text look like?</em></p>
<p>The first step in accomplishing this task is to search the Slate Magazine corpus for GPEs in context. For each occurrence of each GPE in the corpus, then, token and surrounding context are extracted using the <code>corpuslingr::clr_search_context</code> function. Here, context is defined as the 15x15 window of words surrounding a given token of a GPE. We limit our search to the 100 most frequent GPEs.</p>
<pre class="r"><code>gpe_search &lt;- data.frame(slate_gpe_geo) %&gt;%
  arrange(desc(txtf))%&gt;%
  slice(1:100)%&gt;%
  mutate(lemma=paste0(lemma,&#39;~GPE&#39;))</code></pre>
<p>Perform search:</p>
<pre class="r"><code>gpe_contexts &lt;- corpuslingr::clr_search_context(
  search = gpe_search$lemma, 
  corp=slate, 
  LW=15, RW=15)</code></pre>
<p>A small random sample of the search results are presented below in context. The <code>clr_context_kwic</code> function quickly rebuilds the original user-specified search context, with the search term highlighted.</p>
<pre class="r"><code>gpe_contexts %&gt;%
  corpuslingr::clr_context_kwic(include=c(&#39;doc_id&#39;)) %&gt;%
  sample_n(5)%&gt;%
  DT::datatable(class = &#39;cell-border stripe&#39;, 
                rownames = FALSE,
                width=&quot;100%&quot;, 
                escape=FALSE)</code></pre>
<div id="htmlwidget-2" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"filter":"none","data":[["689","321","418","398","401"],["the U.S. return the crew of a fishing boat it says was hijacked out of <mark> Cuba <\/mark> Monday , and is now in Coast_Guard custody . About Face ! On Nov._12 .","and that \" ultimately there will be a calling to account for the rape of <mark> East_Timor <\/mark> . \" In New_Zealand , where world leaders have been discussing the crisis at the_Asia-Pacific_Economic_Cooperation","the shipbuilders , the Citadel cadets , the \" gangstas , \" the_Spur_Posse , the <mark> Vietnam <\/mark> vets , the_Promise_Keepers , the football fans , the militiamen , the porno film stars","'s also a tad bloodless , but you ca n't have everything . Set in <mark> England <\/mark> before World_War_I , the play ( based on an actual incident ) tells the story","are not going to expose their own cities to needless risk . \" Back to <mark> Kosovo <\/mark> : The conservative National_Post_of_Canada ran a tough editorial Wednesday saying that acceptance of Milosevic 's"]],"container":"<table class=\"cell-border stripe\">\n  <thead>\n    <tr>\n      <th>doc_id<\/th>\n      <th>kwic<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="lsa-mds-and-semantic-space" class="section level2">
<h2>LSA, MDS, and semantic space</h2>
<p>So, having extracted all contexts from the corpus, we can now build a GPE-feature matrix (ie, word embeddings by GPE) by applying the <code>clr_context_bow</code> function to the output of <code>clr_search_context</code>. We limit our definition of features to only content words, and aggregate feature frequencies by lemma.</p>
<pre class="r"><code>term_feat_mat &lt;- gpe_contexts %&gt;%
  corpuslingr::clr_context_bow(
    agg_var = c(&#39;searchLemma&#39;,&#39;lemma&#39;),
    content_only=TRUE)%&gt;%
  spread (searchLemma,cofreq)%&gt;%
  replace(is.na(.), 0)</code></pre>
<p>Some of the matrix:</p>
<table>
<thead>
<tr class="header">
<th align="left">lemma</th>
<th align="right">AFGHANISTAN</th>
<th align="right">ALABAMA</th>
<th align="right">ALASKA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">GOT_MAIL</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">GOURMET</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">GOV.</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">GOVERN</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">GOVERNANCE</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">GOVERNMENT</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p><br> Next, we create a cosine-based similarity matrix using the <code>LSA</code> package:</p>
<pre class="r"><code>library(lsa)
sim_mat &lt;- term_feat_mat %&gt;%
  select(2:ncol(term_feat_mat)) %&gt;%
  data.matrix()%&gt;%
  lsa::cosine(.)</code></pre>
<p>The <code>lsa::cosine</code> function computes cosine measures between all GPE vectors of the term-feature matrix. The higher the cosine measure between two vectors, the greater their similarity in composition. The top-left portion of this matrix is presented below:</p>
<pre><code>##             AFGHANISTAN   ALABAMA    ALASKA
## AFGHANISTAN   1.0000000 0.1663644 0.1089837
## ALABAMA       0.1663644 1.0000000 0.1570805
## ALASKA        0.1089837 0.1570805 1.0000000</code></pre>
<p>The last step is to transform the similarities between co-occurrence vectors into two-dimensional space, such that context-based (ie, semantic) similarity is reflected in spatial proximity.</p>
<p>To accomplish this task, we apply classical scaling to the similarity matrix using the base r function <code>cmdscale</code>. Two-dimensional coordinates are then extracted from the <code>points</code> element of the <code>cmdscale</code> output. We join the <code>slate_gpe_details</code> object to the ouput in order to color GPEs in the plot by continent.</p>
<p>As the plot demonstrates, we get a fairly good sense of geo-political space (from the perspective of Slate Magazine contributors) by comparing word embeddings derived from a corpus of only 1 million words.</p>
<pre class="r"><code>cmdscale(1-sim_mat, eig = TRUE, k = 2)$points %&gt;% 
  data.frame() %&gt;%
  mutate (lemma = rownames(sim_mat))%&gt;%
  left_join(slate_gpe_details)%&gt;%
  ggplot(aes(X1,X2)) +
  geom_text(aes(label=colnames(sim_mat),col=continent), 
            size=2.5, 
            check_overlap = TRUE)+
  scale_colour_stata() + theme_fivethirtyeight() +
  theme(legend.position = &quot;none&quot;,
        plot.title = element_text(size=14))+ 
  labs(title=&quot;Slate GPEs in semantic space&quot;,
       subtitle=&quot;A two-dimensional solution&quot;)</code></pre>
<p><img src="/post/2018-01-22-text-geography-and-semantic-space_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>The first dimension (x-axis) seems to do a very nice job capturing a “Domestic - Foreign” distinction, with some obvious exceptions. The second dimension (y-axis) seems to capture a “City - State” distinction, or an “Urban - Non-urban” distinction. Also, there seems to be a “Europe - Non-Europe” element to the second dimension on the “Foreign” side of the plot.</p>
<p>Someone better versed in the geo-political happenings of the waning 20th century could likely provide a more detailed analysis here. Suffice it to say, there is some very intuitive structure to the plot above derived from co-occcurence vectors. While not exclusively geospatial, as a “map” of the geo-political “lay of the land” it certainly has utility.</p>
</div>
<div id="fin" class="section level2">
<h2>FIN</h2>
<p>So, we have weaved together here a set of methodologies that are often discussed in different classrooms, and demonstrated some different approaches to extracting and analyzing the geospatial information contained in text. Maps and “maps.”</p>
</div>
